[[https://classroom.udacity.com/courses/ud007/lessons/3618489075/concepts/last-viewed][ðŸ”—Lecture
on Udacity (2.5 hr)]]

** Branch in a Pipeline
   :PROPERTIES:
   :CUSTOM_ID: branch-in-a-pipeline
   :END:
#+begin_example
  BEQ R1, R2, Label
#+end_example

A branch instruction like this compares R1 and R2, and if equal jumps to
=Label= (usually by having in the immediate part of the instruction
field the difference in PC for the next instruction versus the labeled
instruction, such that it can simply add this offset when branched).

In a 5-stage pipeline, the compare happens in the third (ALU) stage.
Meanwhile, two instructions have moved into Fetch/Read stages. If we
fetched the correct instructions for the branch (or not), they continue
to move correctly with no penalty. Otherwise we must flush and take a
2-instruction penalty.

Thus, it never pays to simply not fetch something as you always take the
penalty, and it is better to take a penalty only sometimes than all the
time. Another important note is that when fetching the instruction after
=BEQ=, we know nothing about the branch itself yet except its address
(we don't even know it's a branch at all yet), but we already must make
a prediction of whether it's a taken branch or not.

** Branch Prediction Requirements
   :PROPERTIES:
   :CUSTOM_ID: branch-prediction-requirements
   :END:

- Using only the knowledge of the instruction's PC

  - Guess PC of next instruction to fetch

- Must correctly guess:

  - is this a branch?
  - is it taken?
  - if taken, what is the target PC?

The first two guesses can be combined into "is this a taken branch?"

*** Branch Prediction Accuracy
    :PROPERTIES:
    :CUSTOM_ID: branch-prediction-accuracy
    :END:
\[ CPI = 1 + \frac{mispred}{inst} * \frac{penalty}{mispred} \]

The \( \frac{mispred}{inst} \) part is determined by the predictor
accuracy. The \(\frac{penalty}{mispred} \) part is determined by the
pipeline (where in the pipeline we figure out the misprediction).

Assumption below: 20% of all instructions are branches (common in
programs).

| Accuracy \(\downarrow\)  | Resolve in 3rd stage        | Resolve in 10th stage       |
|--------------------------+-----------------------------+-----------------------------|
| 50% for BR100% all other | \(1 + 0.5*0.2*2\)\(= 1.2\)  | \(1 + 0.5*0.2*9\)\(= 1.9\)  |
| 90% of BR100% all other  | \(1 + 0.1*0.2*2\)\(= 1.04\) | \(1 + 0.1*0.2*9\)\(= 1.18\) |
| /(Speedup)/              | /1.15/                      | /1.61/                      |

Conclusions: A better branch predictor will help regardless of the
pipeline, but the /amount/ of help changes with the pipeline depth.

*** Performance with Not-Taken Prediction
    :PROPERTIES:
    :CUSTOM_ID: performance-with-not-taken-prediction
    :END:
|            | Refuse to Predict(stall until sure) | Predict Not-Taken(always increment PC) |
|------------+-------------------------------------+----------------------------------------|
| Branch     | 3 cycles                            | 1 or 3 cycles                          |
| Non-Branch | 2 Cycles                            | 1 cycle                                |

Thus, Predict Not-Taken always wins over refusing to predict

*** Predict Not-Taken
    :PROPERTIES:
    :CUSTOM_ID: predict-not-taken
    :END:
Operation: Simply increment PC (no special hardware or memory, since we
have to do this anyway)

Accuracy: * 20% of instructions are branches * 60% of branches are taken
* \(\Rightarrow\) Correctness: 80% (non-branches) + 8% (non-taken
branches) * \(\Rightarrow\) Incorrect 12% of time * CPI = 1 +
0.12*penalty

** Why We Need Better Prediction?
   :PROPERTIES:
   :CUSTOM_ID: why-we-need-better-prediction
   :END:
|                          | Not Taken88%                     | Better99%                        | Speedup  |
|--------------------------+----------------------------------+----------------------------------+----------|
| 5-stages(3rd stage)      | \(1 + 0.12*2\)\(CPI = 1.24\)     | \(1 + 0.01*2\)\(CPI = 1.02\)     | \(1.22\) |
| 14-stages(11th stage)    | \(1 + 0.12*10\)\(CPI = 2.2\)     | \(1 + 0.01*10\)\(CPI = 1.1\)     | \(2\)    |
| 11th stage(4 inst/cycle) | \(0.25 + 0.12*10\)\(CPI = 1.45\) | \(0.25 + 0.01*10\)\(CPI = 0.35\) | \(4.14\) |

If we have a deeper pipeline or are able to execute more instructions
per cycle, the better predictor is more important than in simpler
processors, because the cost of misprediction is much higher (more
instructions lost with a misprediction).

*** Better Prediction - How?
    :PROPERTIES:
    :CUSTOM_ID: better-prediction---how
    :END:
Predictor must compute \(PC_{next}\) based only on knowledge of
\(PC_{now}\). This is not much information to decide on. It would help
if we knew: * Is it a branch? * Will it be taken? * What is the offset
field of the instruction?

But we don't know any of these because we're still fetching the
instruction. We do, however, know the history of how \(PC_{now}\) has
behaved in the past. So, we can go from: \(PC_{next} = f(PC_{now})\) to:

\[ PC_{next} = f(PC_{now}, history[PC_{now}]) \]

** BTB - Branch Target Buffer
   :PROPERTIES:
   :CUSTOM_ID: btb---branch-target-buffer
   :END:
The predictor can take the \(PC_{now}\) and uses it to index into a
table called the BTB, with the output of our best guess at next PC.
Later, when the branch executes, we know the actual \(PC_{next}\) and
can compare with the predicted one. If it doesn't match, then it is
handled as a misprediction and the BTB can be updated.

#+caption: Branch Target Buffer
[[https://i.imgur.com/h6Fwke1.png]]

One problem: How big does the BTB need to be? We want it to have single
cycle latency (small). However, it needs to contain an entire 64-bit
address and we one entry for each PC we can fetch. Thus, the BTB would
need to be as large as the program itself. How do we make it smaller?

*** Realistic BTB
    :PROPERTIES:
    :CUSTOM_ID: realistic-btb
    :END:
First, we don't need an entry for every possible PC. It's enough if we
have entries for any PC likely to execute soon. For example, in a loop
of 100 instructions, it's enough to have about a 100-length BTB.

Perhaps we find through testing that a 1024-entry BTB can still be
accessed in one cycle. How do we map 64-bit PCs to this 1024 entry
table, keeping in mind any delay in calculating the mapping from PC to
BTB index would then shorten the BTB further to compensate.

The way we do this is by simply taking the last 10 bits (in the 1024
case). While this means future instructions will eventually overwrite
the BTB entries for the current instructions, this ensures instructions
located around each other are all mapping to the BTB during execution.
This particularly applies to better predicting branch behavior in loops
or smaller programs.

If instructions are word-aligned, the last two bits will always be
=0b00=. Therefore we can ignore those and index using bits 12-2 in the
1024 case.

** Direction Predictor
   :PROPERTIES:
   :CUSTOM_ID: direction-predictor
   :END:
The BHT is used like the BTB, but the entry is a single bit that tells
us whether a PC is: - [0] not a taken branch (PC++) - [1] a taken branch
(use BTB)

The entries are accessed by least insignificant bits of PC (e.g.Â 12-2 in
1024 case). Once the branch resolves we can update BHT accordingly.
Because this table is very small in terms of data, it can be much larger
in terms of entries.

*** Problems with 1-bit Predictor
    :PROPERTIES:
    :CUSTOM_ID: problems-with-1-bit-predictor
    :END:
It works well if an instruction is always taken or else always not
taken. It also predicts well when taken branches >>> not taken, or not
taken >>> taken. Every "switch" (anomaly) between an instruction being
taken and not taken causes two mispredictions (on the change, and then
the next change).

So, the 1-bit predictor does not do so well if the taken to not taken
ratio is not very large. It also will not perform well in short because
this same anomaly occurs when the loop is executed again (the previous
loop exit will cause it to mispredict the new loop).

** 2-Bit Predictor (2BP, 2BC)
   :PROPERTIES:
   :CUSTOM_ID: bit-predictor-2bp-2bc
   :END:
This predictor fixes the behavior of the 1-bit predictor during the
anomaly. The upper bit behaves like the 1-bit predictor, but it adds a
hysteresis (or "conviction") bit.

#+caption: 1-bit predictor state machine
[[https://i.imgur.com/pe7RYc9.png]]

| Prediction Bit | Hysteresis Bit | Description      |
|----------------+----------------+------------------|
| 0              | 0              | Strong Not-Taken |
| 0              | 1              | Weak Not-Taken   |
| 1              | 0              | Weak Taken       |
| 1              | 1              | Strong Taken     |

Basically, the upper bit controls the final prediction, but the lower
bit allows the predictor to flow towards the opposite prediction. Thus a
=0b00= state would require 2 taken branches in a row to change its
prediction toward a taken branch. This prevents the case of a single
anomaly causing multiple mispredictions, in that the behavior itself
must be changing for the prediction to change.

#+caption: 2-bit predictor state machine
[[https://i.imgur.com/TbSV6zv.png]]

It can be called a 2-bit counter because it simply increments on taken
branches, or decrements on not-taken. This allow the predictor to be
easily implementable.

*** 2-Bit Predictor Initialization
    :PROPERTIES:
    :CUSTOM_ID: bit-predictor-initialization
    :END:
The question is - where do we start with the predictor? If we start in a
strong state and are correct, then we have no mispredictions. If we are
wrong, it costs two mispredictions before it corrects itself. If we
start in a weak state and are right, we also have perfect prediction.
However, if we were wrong, it only costs a single misprediction before
it corrects itself. This leads us to think it's always best to start in
a weak state.

However, consider the state where a branch flips between Taken and
Not-Taken; by starting in a strong state, we mispredict half the time.
Starting in a weak state, we /constantly/ mispredict.

#+caption: 2-bit predictor misprediction on flipped state
[[https://i.imgur.com/LcNJv41.png]]

So, while it may seem better to start in one state or another, in
reality there is no way to predict what state is best for the incoming
program, and indeed there is always some sequence of taken/not-taken
that can result in constant misprediction. Therefore, it is best to
simply initialize in the easiest state to start with, typically =0b00=.

*** 1-bit to 2-bit is Good... 3-bit, 4-bit?
    :PROPERTIES:
    :CUSTOM_ID: bit-to-2-bit-is-good-3-bit-4-bit
    :END:
If there is benefit in moving to 2-bit, what about just adding more
bits? This really serves to increase the cost (larger BHT to serve all
PCs), but is only good when anomalous outcomes happen in streaks. How
often does this happen? Sometimes. Maybe 3 bits might be worth it, but
likely not 4. Best to stick with 2BP.

** History-Based Predictors
   :PROPERTIES:
   :CUSTOM_ID: history-based-predictors
   :END:
These predictors function best with a repeatable pattern
(e.g.Â "N-T-N-T-N-T..." or "N-N-T-N-N-T..."). These are 100% predictable,
just not with simple n-bit counters. So, how do we learn the pattern?

#+caption: history-based predictor
[[https://i.imgur.com/QFxW4dB.png]]

In this case our prediction is done by looking "back" at previous steps.
So in the first pattern, we know when the history is N, predict T, and
vice-versa. In the second pattern, we look back two steps. So when the
history is NN, predict T. When it is NT, predict N. And when it is TN,
predict N.

*** 1-bit History with 2-bit Counters
    :PROPERTIES:
    :CUSTOM_ID: bit-history-with-2-bit-counters
    :END:
In this predictor, each BHT entry has a single history bit, and two
2-bit counters. The history bit can then be used to index into which
counter to use for the prediction.

#+caption: 1-bit history with 2-bit counters
[[https://i.imgur.com/wn9dPMD.png]]

While this type of predictor works great for patterns like this, it
still mispredicts 1/3 of the time in a "NNT-NNT-NNT" type pattern, as
the prediction following an N is a 50% chance of being right.

*** 2-bit History Predictor
    :PROPERTIES:
    :CUSTOM_ID: bit-history-predictor
    :END:
This predictor works the same way as the 1-bit history predictor, but
now we have 2 bits of history used to index into a 2BC[4] array. This
perfectly predicts both the (NT)* and (NNT)* pattern types and is a
pretty good predictor for other patterns. However, it wastes one 2BC on
the (NNT)* case and two 2BCs on the (NT)* case.

#+caption: 2-bit history predictor
[[https://i.imgur.com/7WaZWRg.png]]

*** N-bit History Predictor
    :PROPERTIES:
    :CUSTOM_ID: n-bit-history-predictor
    :END:
We can generalize to state that an N-bit history predictor can
successfully predict all taken patterns of \(length \leq N+1\), but will
cost \(N+2*2^N\) bits per entry and waste most 2BCs. So, while
increasing N will give us the ability to predict longer patterns, we do
so at rapidly increasing cost with more waste.

** History-Based Predictors with Shared Counters
   :PROPERTIES:
   :CUSTOM_ID: history-based-predictors-with-shared-counters
   :END:
Instead of \(2^N\) counters per entry, we want to use \(\approx N\)
counters. The idea is to share 2BCs between entries instead of each
entry having its own counter.

We can do this with a Pattern History Table (PHT). This table simply
keeps some PC-indexed history bits (N bits per entry), combines that
with bits of the PC (XOR) to index into the BHT, each entry of which is
just a single 2BC. Thus it is very possible to have two entries/history
combinations using the same BHT entry

#+caption: history with shared counters
[[https://i.imgur.com/7PeYo5u.png]]

Thus, small patterns will only use a few counters, leaving many other
counters for longer, more complex patterns to use. This can still result
in wasted space, but not nearly as much as the exponential increase of
the N-bit history predictor. The downside, of course, is that some
branches with particular histories may overlap with other
branches/histories. But, if the BHT is large enough (and it can be with
each entry being a single 2BC), this should happen rarely.

*** PShare and GShare Predictors
    :PROPERTIES:
    :CUSTOM_ID: pshare-and-gshare-predictors
    :END:
This shared counters predictor is called PShare -> "P"rivate history,
"Share"d counters. This is good for even-odd and 8-iteration loops.

Another option is GShare, or "G"lobal history, "Share"d counters. The
history indexes are shared among all entries, and the PC+History
operation (XOR) is used to index into the BHT. This is good for
correlated branches - which is very common in programs, as typically
operations in one branch are probably somewhat dependent on operations
from a previous branch.

Which to use? Both! Use GShare for correlated branches, and PShare for
single branches with shorter history.

** Tournament Predictor
   :PROPERTIES:
   :CUSTOM_ID: tournament-predictor
   :END:
We have two predictors, PShare and GShare, each of which is better at
predicting certain types of branches. A meta-predictor (array of 2BCs)
is used not as a branch predictor, but rather as a predictor of which
other predictor is more likely to yield a correct prediction for the
current branch. At each step, you "train" each individual predictor
based on the outcome, and you also train the meta-predictor on how well
each predictor is doing.

#+caption: tournament predictor
[[https://i.imgur.com/Pa56hP0.png]]

** Hierarchical Predictor
   :PROPERTIES:
   :CUSTOM_ID: hierarchical-predictor
   :END:
Like a tournament predictor, but instead of combining two good
predictors, it is using one good and one ok predictor. The idea is that
good predictors are expensive, and some branches are very easy to
predict. So the "ok" predictor can be used for these branches, and the
better predictor can be saved for the branches that are more difficult
to predict.

|            | Tournament                                                                  | Hierarchical                                                              |
|------------+-----------------------------------------------------------------------------+---------------------------------------------------------------------------|
| Predictors | 2 good predictors                                                           | 1 good, 1 ok                                                              |
| Updates    | Update both for every decision                                              | Update OK-pred on every decisionUpdate Good-pred only if OK-pred not good |
| (Other)    | Good predictors are both chosen to be balanced between performance and cost | Can use a combination of predictors of differing quality                  |

In this example, the 2BC simple predictor will be used for most
branches, but if it is doing a poor job the branch is added to the Local
predictor. Similarly it could also go to the Global predictor. Over time
the CPU is "trained" on how to handle each branch.
[[https://i.imgur.com/o8jriH3.png]]

** Return Address Stack (RAS)
   :PROPERTIES:
   :CUSTOM_ID: return-address-stack-ras
   :END:
For any branch, we need to know the direction (taken, not taken) and the
target. The previous predictors have covered direction (BHT) and target
(BTB) find for most types of branches (complex like =BEQ= and =BNE=,
simple like =JUMP= and =CALL=, etc.). However, there is a type of
branch, the function return, which is always taken (so direction
prediction is fine), but the target is more difficult to predict, as it
can be called from many different places. The BTB is not good at
predicting the target when it could change each time.

The RAS is a predictor dedicated to predicting function returns. The
idea is that upon each function call, we push the return address (PC+4)
on the RAS. When we return, we pop from this stack to get the correct
target address.

Why not simply use the stack itself? The prediction should be very close
to the other predictors, and using a separate stack allows the predictor
to be very small hardware structure and make the prediction very
quickly.

What happens if we exceed the size of the RAS? Two choices: - Don't
push - Wrap around - this is the best approach (main and top level
functions do not consume the entire RAS)

In the end, remember this is another predictor and we are allowed to
have mispredictions - the stack is still there and will work, just with
misprediction cost. The goal is to optimize the greatest number of
branches and returns, so the wrap-around approach is best.

*** How do we know it's a =RET=?
    :PROPERTIES:
    :CUSTOM_ID: how-do-we-know-its-a-ret
    :END:
This is all still during the IF phase, so how do we even know if the
instruction is a =RET=? One simple way is to use a single-bit predictor
to whether an instruction is a =RET= or not. This is very accurate (that
PC is likely to always be a =RET=).

Another approach is called "Predecoding". Most of the time instructions
are coming from the processor cache and are only loaded from memory when
not in the cache. This strategy involves storing part of the decoded
instruction along with the instruction in cache. For example, if an
instruction is 32-bits, maybe we store 33 bits, where the extra bit
tells us if it is a =RET= or not. The alternative is decoding this every
time the instruction is fetched, which becomes more expensive. Modern
processors store a lot of information during the predecode step such
that the pipeline can move quickly during execution.

/[2BP]: 2-bit Predictor /[2BC]: 2-Bit Counter /[2BCs]: 2-Bit Counters
/[ALU]: Arithmetic Logic Unit /[BHT]: Branch History Table /[BTB]:
Branch Target Buffer /[CPI]: Cycles Per Instruction /[IF]: Instruction
Fetch /[PC]: Program Counter /[PHT]: Pattern History Table /[RAS]:
Return Address Stack /[XOR]: Exclusive-OR
