[[https://classroom.udacity.com/courses/ud007/lessons/872590122/concepts/last-viewed][ðŸ”—Lecture
on Udacity (1.5hr)]]

** Dependability
   :PROPERTIES:
   :CUSTOM_ID: dependability
   :END:
Quality of delivered service that justifies relying on the system to
provide that service. - Specified Service = what behavior should be -
Delivered Service = actual behavior

System has components (modules) - Each module has an ideal specified
behavior

** Faults, Errors, and Failures (+ Example)
   :PROPERTIES:
   :CUSTOM_ID: faults-errors-and-failures-example
   :END:

- *Fault* - module deviates from specified behavior

  - Example: Programming mistake

    - Add function that works fine, except 5+3 = 7
    - Latent Error (only a matter of time until activated)

- *Error* - actual behavior within system differs from specified
  behavior

  - Example: (Activated Fault, Effective Error)

    - We call add() with 5 and 3, get 7, and put it in some variable

- *Failure* - System Deviates from specified behavior

  - Example: 5+3=7 -> Schedule a meeting for 7am instead of 8am

An error starts with a fault, but a fault may not necessarily become an
error. A failure starts with an error, but an error may not necessarily
result in a failure. Example: =if(add(5, 3))>0)= causes an error since
the fault is activated, but not a failure since the end result is not
affected.

** Reliability, Availability
   :PROPERTIES:
   :CUSTOM_ID: reliability-availability
   :END:
System is in one of two states: * Service Accomplishment * Service
Interruption

Reliability: * Measure continuous Service Accomplishment * Mean Time to
Failure (MTTF)

Availability: * Service Accomplishment as a fraction of overall time *
Need to know: Mean Time to Repair (MTTR) * Availability =
\(\frac{MTTF}{MTTF+MTTR}\)

** Kinds of Faults
   :PROPERTIES:
   :CUSTOM_ID: kinds-of-faults
   :END:
By Cause: * HW Faults - HW fails to perform as designed * Design
Faults - SW bugs, HW design mistakes (FDIV bug) * Operation Faults -
Operator, user mistakes * Environmental Faults - Fire, power failures,
sabotage, etc.

By Duration: * Permanent - Once we have it, it doesn't get corrected
(wanted to see what's inside processor, and now it's in 4 pieces) *
Intermittent - Last for a while, but recurring (overclock - temporary
crashes) * Transient - Something causes things to stop working
correctly, but it fixes itself eventually

** Improving Reliability and Availability
   :PROPERTIES:
   :CUSTOM_ID: improving-reliability-and-availability
   :END:

- Fault Avoidance

  - Prevent Faults from occurring at all
  - Example: No coffee in server room

- Fault Tolerance

  - Prevent Faults from becoming Failures
  - Example: Redundancy, e.g.Â ECC for memory

- Speed up Repair Process (availability only)

  - Example: Keep a spare hard disk in drawer

** Fault Tolerance Techniques
   :PROPERTIES:
   :CUSTOM_ID: fault-tolerance-techniques
   :END:

- Checkpointing (Recover from error)

  - Save state periodically
  - If we detect errors, restore the saved state
  - Works well for many transient and intermittent failures
  - If this takes too long, it has to be treated like a service
    interruption

- 2-Way Redundancy (Detect error)

  - Two modules do the same work and compare results
  - Roll back if results are different

- 3-Way Redundancy (Detect and recover from error)

  - 3 modules (or more) do the same work and vote on correctness
  - Fault in one module does not become an error overall.
  - Expensive - 3x the hardware required, but can tolerate /any/ fault
    in one module

** N-Module Redundancy
   :PROPERTIES:
   :CUSTOM_ID: n-module-redundancy
   :END:

- N=2 - Dual-Module Redundancy

  - Detect but not correct one faulty module

- N=3 - Triple-Module Redundancy

  - Correct one faulty module

- N=5 - (example: space shuttle)

  - 5 computers perform operation and vote
  - 1 Wrong Result \(\Rightarrow\) normal operation
  - 2 Wrong Results \(\Rightarrow\) abort mission

    - Still no failure from this: 3 outvote the 2

  - 3 Wrong Results: failure can be catastrophic (too many broken
    modules)

    - Abort with 2 failures so that this state should never be reached

** Fault Tolerance for Memory and Storage
   :PROPERTIES:
   :CUSTOM_ID: fault-tolerance-for-memory-and-storage
   :END:

- Dual/Triple Module Redundancy - Overkill (typically better for
  computation)
- Error Detection, Correction Codes

  - Parity: One extra bit (XOR of all data bits)

    - Fault flips one bit \(\Rightarrow\) Parity does not match data

  - ECC: example - SECDED (Single Error Correction, Double Error
    Detection)

    - Can detect and fix any single bit flip, or can only detect any
      dual bit flip
    - Example: ECC DRAM modules

  - Disks can use even fancier codes (e.g.Â Reed-Solomon)

    - Detect and correct multiple-bit errors (especially streaks of
      flipped bits)

- RAID (for hard disks)

** RAID - Redundant Array of Independent Disks
   :PROPERTIES:
   :CUSTOM_ID: raid---redundant-array-of-independent-disks
   :END:

- Several disks playing the role of one disk (can be larger and/or more
  reliable than the single disk)
- Each disk detects errors using codes

  - We know which disk has the error

- RAID should have:

  - Better performance
  - Normal Read/Write accomplishment even when:

    - It has a bad sector
    - Entire disk fails

- RAID 0, 1, etc...

*** RAID 0: Striping (to improve performance)
    :PROPERTIES:
    :CUSTOM_ID: raid-0-striping-to-improve-performance
    :END:
Disks can only read one track at a time, since the head can be in only
one position. RAID 0 takes two disks and "stripes" the data across each
disk such that consecutive tracks can be accessed simultaneously with
the head in a single position. This results in up to 2x the data
throughput and reduced queuing delay.

However, reliability is worse than a single disk: * \(f\) = failure rate
for a single disk * failures/disk/second * Single-Disk MTTF =
\(\frac{1}{f}\) (MTTDL: Mean time to data loss = \(MTTF_1\)) * N disks
in RAID0 * \(f_N = N/f_1 \Rightarrow MTTF_N = MTTDL_N =
\frac{MTTF_1}{N}\) / 2 Disks \( \Rightarrow  MTTF_2 = \frac{MTTF_1}{2}
\)

*** RAID 1: Mirroring (to improve reliability)
    :PROPERTIES:
    :CUSTOM_ID: raid-1-mirroring-to-improve-reliability
    :END:
Same data on both disks * Write: Write to each disk * Same performance
as 1 disk alone * Read: Read any one disk * 2x throughput of one disk
alone * Can tolerate any faults that affect one disk * Two copies ->
Detect Error ??? * Not true in this case, because ECC on each sector
lets us know which one has a fault

Reliability: * \(f\) = failure rate for a single disk *
failures/disk/second * Single-Disk MTTF = \(\frac{1}{f}\) (MTTDL: Mean
time to data loss = \(MTTF_1\)) * 2 disks in RAID1 * \(f_N = N/f_1
\Rightarrow\) / both disks OK until \(\frac{MTTF_1}{2}\) * remaining
disk lives on for \(MTTF_1\) time * \(MTTDL_{RAID1-2} =
\frac{MTTFF_1}{2}+MTTF_1\) (Assumes no disk replaced) * But we do
replace failed disks! * Both disks ok until \(\frac{MTTF_1}{2}\) * Disk
fails, have one OK disk for \(MTTR_1\) * Both disks ok again until
\(\frac{MTTF_1}{2}\) * So, overall MTTDL : * (when \(MTTR_1
\ll MTTF_1\), probability of second disk failing during MTTR =
\(\frac{MTTR_1}{MTTF_1}\)) * \(MTTDL_{RAID1-2} = \frac{MTTF_1}{2} *
\frac{MTTF_1}{MTTR_1}\) (second factor is 1/probability)

*** RAID 4: Block-Interleaved Parity
    :PROPERTIES:
    :CUSTOM_ID: raid-4-block-interleaved-parity
    :END:

- N disks

  - N-1 contain data, striped like RAID 0
  - 1 disk has parity blocks

| Disk 0   |   | Disk 1   |   | Disk 2   |   | Disk 3       |
|----------+---+----------+---+----------+---+--------------|
| Stripe 0 | âŠ• | Stripe 1 | âŠ• | Stripe 2 | = | Parity 0,1,2 |
| Stripe 3 | âŠ• | Stripe 4 | âŠ• | Stripe 5 | = | Parity 3,4,5 |

Data from each stripe is XOR'd together to result in parity information
on disk 3. If any one of the disks fail, the data can then be
reconstructed by XOR-ing all the other disks, including parity.

- Write: Write 1 data disk and parity disk read/write
- Read: Read 1 disk

Performance and Reliability: * Reads: throughput of N-1 disks * Writes:
1/2 throughput of single disk (primary reason for RAID 5) * MTTF: * All
disks ok for \(\frac{MTTF_1}{N}\) * If no repair, we are left with N-1
disk array: + \(\frac{MTTF_1}{N-1} \rightarrow\) Bad idea * Repair
\(\Rightarrow\) chance of another failing during repair:
\(\frac{MTTF_1}{N-1} \): Multiply by this factor over \(MTTR_1\) *
\(MTTF_{RAID4} = \frac{MTTF_1 \* MTTF_1}{N\*(N-1)\*MTTR_1}\)

Reads: [[https://www.youtube.com/watch?v=3QXaSzM2fE8][ðŸŽ¥ View Lecture
Video (2:19)]] - If we compute the XOR of the old vs new data we are
writing, we get the bit flips we're making to the data. If we then XOR
this against the parity block, we perform those same bit flips on the
parity data and get the new parity information. - Thus, the parity disk
is a bottleneck for writes (multiple disks may be updating it)
\(\Rightarrow\) RAID5

*** RAID 5: Distributed Block-Interleaved Parity
    :PROPERTIES:
    :CUSTOM_ID: raid-5-distributed-block-interleaved-parity
    :END:

- Like RAID 4, but parity is spread among all disks: | Disk 0 | | Disk 1
  | | Disk 2 | | Disk 3 |
  |----------|---|-----------|---|-----------|---|-------------| |
  Stripe 0 | âŠ• | Stripe 1 | âŠ• | Stripe 2 | = | Parity 0,1,2 | | Parity
  3,4,5 | = | Stripe 3 | âŠ• | Stripe 4 | âŠ• | Stripe 5 | | Stripe 6 | |
  Parity 6,7,8 | | Stripe 7 | | Stripe 8 | | Stripe 9 | | Stripe 10 | |
  Parity 9,10,11 | | Stripe 11| | Stripe 12 | âŠ• | Stripe 13 | âŠ• | Stripe
  14 | = | Parity 12,13,14 |
- Read Performance: N * throughput of 1 disk
- Write Performance: 4 accesses/write, but distributed: N/4 * throughput
  of 1
- Reliability: same as RAID4 - if we lose any one disk, we have a
  problem
- Capacity: same as RAID4 (still sacrifice one disk worth of parity over
  the array)

Key takeaway: We should always choose RAID5 over RAID4, since we lose
nothing with reliability or capacity, and gain in throughput, all
without additional hardware requirements.

*** RAID 6?
    :PROPERTIES:
    :CUSTOM_ID: raid-6
    :END:

- Two "parity" blocks per group

  - Can work when 2 failed stripes/group
  - One parity block
  - Second is a different type of check-block
  - When 1 disk fails, use parity
  - When 2 disks fail, solve equations to retrieve data

- RAID 5 vs.Â RAID 6

  - 2x overhead
  - More write overhead (6/WR vs 4/WR)
  - Only helps reliability when disk fails, then another fails before we
    replace the first one (low probability, thousands of years MTTDL)

RAID6 is an overkill? * RAID5: disk fails, 3 days to replace * Very low
probability of another failing in those 3 days (assuming independent
failure) * Failures can be related! * RAID5, 5 disks, 1 disk fails (#2)
* System says "Replace disk #2" * Operator gets replacement disk and
inserts it into spot 2 * But... numbering was 0,1,2,3,4! Operator pulled
the wrong disk. * Now we have two failed disks (one hard failure, one
operator error) * RAID6 prevents both a single or double disk failure.

The above scenario sounds silly, but with very long MTTF values,
replacing a RAID disk becomes a rare activity, and is prone to operator
error. A real-life personal scenario I (author of these notes)
encountered was not properly grounding myself when replacing a disk in a
hot-swappable RAID5. When inserting it into the housing, my theory is
that I discharged static electricity into the disk below the one I was
replacing, and the array failed to rebuild due to an error on that disk.
Thankfully we were able to move the platters from that drive into a new
one and the array rebuilt ok, avoiding the need to use week-old backups.
So, RAID6 is indeed a bit overkill, but could have prevented this
scenario, which resulted in lots of system downtime.

/[MTTDL]: Mean Time to Data Loss /[MTTF]: Mean Time to Failure *[MTTR]:
Mean Time to Repair
