[[https://classroom.udacity.com/courses/ud007/lessons/1097109180/concepts/last-viewed][ðŸ”—Lecture
on Udacity (1.25hr)]]

** Flynn's Taxonomy of Parallel Machines
   :PROPERTIES:
   :CUSTOM_ID: flynns-taxonomy-of-parallel-machines
   :END:

- How many instruction streams
- How many data streams

|                         | Instruction Streams | Data Streams |                                                             |
|-------------------------+---------------------+--------------+-------------------------------------------------------------|
| SISD(uniprocessor)      | 1                   | 1            | Single Instruction Single Data                              |
| SIMD(vector, SSE/MMX)   | 1                   | > 1          | Single Instruction Multiple Data                            |
| MISD(stream processor?) | > 1                 | 1            | Multiple Instruction Single Data (rare)                     |
| MIMD(multiprocessor)    | > 1                 | > 1          | Multiple Instruction Multiple Data (most today, multi-core) |

** Why Multiprocessors?
   :PROPERTIES:
   :CUSTOM_ID: why-multiprocessors
   :END:

- Uniprocessors already \(\approx\) 4-wide

  - Diminishing returns from making it even wider
  - Faster?? \(frequency \uparrowÂ \Rightarrow voltage
    \uparrowÂ \Rightarrow power \uparrow^3Â \Rightarrow (fire)\)

- But Moore's Law continues!

  - 2x transistors every 18 months
  - \(\RightarrowÂ \approx\)2x performance every 18 months (assuming we
    can use all the cores)

** Multiprocessor Needs Parallel Programs!
   :PROPERTIES:
   :CUSTOM_ID: multiprocessor-needs-parallel-programs
   :END:

- Sequential (single-threaded) code is /a lot/ easier to develop
- Debugging parallel code is /much/ more difficult
- Performance scaling /very/ hard to get

  - (as we scale # cores, performance scales equally, linearly)
  - In reality, performance will improve to a point, but then levels
    off. We can keep improving the program, but it will still max out at
    some level of performance no matter how many cores.

** Centralized Shared Memory
   :PROPERTIES:
   :CUSTOM_ID: centralized-shared-memory
   :END:
Each core has its own cache, but connected to the same bus that shares
main memory and I/O. Cores can send data to each other by writing to a
location in main memory. Similarly they can share I/O. This architecture
is called a Symmetric Multiprocessor (SMP), another concept is Uniform
Memory Access (UMA), in terms of time.
[[https://i.imgur.com/27aePNH.png]]

*** Centralized Main Memory Problems
    :PROPERTIES:
    :CUSTOM_ID: centralized-main-memory-problems
    :END:

- Memory Size

  - Need large memory \(\Rightarrow\) slow memory

- Memory Bandwidth

  - Misses from all cores \(\Rightarrow\) memory bandwidth contention

    - As we add more cores, it starts to serialize against memory
      accesses, so we lose performance benefit

Works well only for smaller machines (2, 4, 8, 16 cores)

** Distributed +Shared+ Memory
   :PROPERTIES:
   :CUSTOM_ID: distributed-shared-memory
   :END:
This uses message passing for a core to access another core's memory. If
accesses are not in the local cache or the memory slice local to the
core, it actually sends a request for this memory through the network to
the core that has it. This system is also called a cluster or a
multi-computer, since each core is somewhat like an individual computer.
* [[https://i.imgur.com/jIRNFwL.png]]

These computers tend to scale better - not because network communication
is faster, but rather because the programmer is forced to explicitly
think about distributed memory access and will tend to minimize
communication.

** A Message-Passing Program
   :PROPERTIES:
   :CUSTOM_ID: a-message-passing-program
   :END:
#+begin_src cpp
  #define ASIZE 1024
  #define NUMPROC 4
  double myArray[ASIZE/NUMPROC]; // each processor allocates only 1/4 of entire array (assumes elements have been distributed somehow)
  double mySum = 0;
  for(int i=0; i < ASIZE/NUMPROC; i++)
      mySum += myArray[i];
  if(myPID==0) {
      for(int p=1; p < NUMPROC; p++) {
          int pSum;
          recv(p, pSum);  // receives the sums from the other processors
          mySum += pSum;
      }
      printf("Sum: %lf\n", mySum);
  } else {
      send(0, mySum);     // sends the local processor's sum to p0
  }
#+end_src

** A Shared Memory Program
   :PROPERTIES:
   :CUSTOM_ID: a-shared-memory-program
   :END:
Benefit: No need to distribute the array!

#+begin_src cpp
  #define ASIZE 1024
  #define NUMPROC 4
  shared double array[ASIZE]; // each processor needs the whole array, but it is shared
  shared double allSum = 0;   // total sum in shared memory
  shared mutex sumLock;       // shared mutex for locking
  double mySum = 0;
  for(int i=myPID*ASIZE/NUMPROC; i < (myPID+1)*ASIZE/NUMPROC; i++) {
      mySum += array[i];      // sums up this processor's own part of the array
  }
  lock(sumLock);              // lock mutex for shared memory access
  allSum += mySum;
  unlock(sumLock);            // unlock mutex
  // <<<<< Need a barrier here to prevent p0 from displaying result until other processes finish
  if(myPID==0) {
      printf("Sum: %lf\n", allSum);
  }
#+end_src

** Message Passing vs Shared Memory
   :PROPERTIES:
   :CUSTOM_ID: message-passing-vs-shared-memory
   :END:
|                   | Message Passing | Shared Memory  |
|-------------------+-----------------+----------------|
| Communication     | Programmer      | Automatic      |
| Data Distribution | Manual          | Automatic      |
| Hardware Support  | Simple          | Extensive      |
| Programming       |                 |                |
| - Correctness     | Difficult       | Less Difficult |
| - Performance     | Difficult       | Very Difficult |

Key difference: Shared memory is easier to get a correct solution, but
there may be a lot of issues making it perform well. With Message
Passing, typically once you've gotten a correct solution you also have
solved a large part of the performance problem.

** Shared Memory Hardware
   :PROPERTIES:
   :CUSTOM_ID: shared-memory-hardware
   :END:

- Multiple cores share physical address space

  - (all can issue addresses to any memory address)
  - Examples: UMA, NUMA

- Multi-threading by time-sharing a core

  - Same core \(\rightarrow\) same physical memory
  - (not really benefiting from multi-threading)

- Hardware multithreading in a core

  - Coarse-Grain: change thread every few cycles
  - Fine-Grain: change thread every cycle (more HW support)
  - Simultaneous Multi-Threading (SMT): any given cycle we can be doing
    multiple instructions from different threads

    - Also called Hyperthreading

** Multi-Threading Performance
   :PROPERTIES:
   :CUSTOM_ID: multi-threading-performance
   :END:
[[https://www.youtube.com/watch?v=ZpqeeHFWxes][ðŸŽ¥ View lecture video
(8:25)]]

Without multi-threading, processor activity in terms of width is very
limited - there will be many waits in which no work can be done. As it
switches from thread to thread (time sharing), the activity of each
thread is somewhat sparse.

In a Chip Multiprocessor (CMP), each core runs different threads. While
the activity happens simultaneously in time, each core still has to deal
with the sparse workload created by the threads it is running. (Total
cost 2x)

With Fine-Grain Multithreading, we have one core, but with separate sets
of registers for different threads. Each thread's work switches from
cycle to cycle, taking advantage of the periods of time in which a
thread is waiting on something to be ready to do the next work. This
keeps the core busier overall, at the slight expense of extra hardware
requirements. (Total cost \(\approx\) 1.05x)

With SMT, we can go one step further by mixing instructions from
different threads into the same cycle. This dramatically reduces the
total idle time of the processor. This requires much more hardware to
support. (Total cost \(\approx\) 1.1x)

** SMT Hardware Changes
   :PROPERTIES:
   :CUSTOM_ID: smt-hardware-changes
   :END:

- Fetch

  - Need additional PC to keep track of another thread

- Decode

  - No changes needed

- Rename

  - Renamer does the same thing as per one thread
  - Need another RAT for the other thread

- Dispatch/Execution

  - No changes needed

- ROB

  - Need a separate ROB with its own commit point
  - Alternatively: interleave in single ROB to save HW expense (more
    often in practice)

- Commit

  - Need a separate ARF for the other thread.
    [[https://i.imgur.com/4fC0ULd.png]]

Overall: cost is not that much higher - the most expensive parts do not
need duplication to support SMT.

** SMT, D$, TLB
   :PROPERTIES:
   :CUSTOM_ID: smt-d-tlb
   :END:
With a VIVT cache, we simply send the virtual address into the cache and
use the data that comes out. However, each thread may have different
address spaces, so the cache has no clue which of the two it is getting.
So with VIVT, we risk getting the wrong data!

With a VIPT cache (w/o aliasing), the same virtual address will be
combined with the physical address and things will work correctly as
long as the TLB provides the right information. So for VIPT and PIPT
both, SMT addressing will work fine as long as the TLB is thread-aware.
One way to ensure this is with a thread bit on each TLB entry, and to
pass the thread ID into the TLB lookup.
[[https://i.imgur.com/6zKXp5i.png]]

** SMT and Cache Performance
   :PROPERTIES:
   :CUSTOM_ID: smt-and-cache-performance
   :END:

- Cache on the core is shared by all SMT threads

- Fast data sharing (good)

  #+begin_example
    Thread 0: SW A
    Thread 1: LW A <--- really good performance
  #+end_example

- Cache capacity (and associativity) shared (bad)

  - If WS(TH0) + WS(TH1) - WS(TH0, TH1) > D$ size:\\
    \(\Rightarrow\) Cache Thrashing
  - If WS(TH0) < D$ size\\
    \(\Rightarrow\) SMT performance can be worse than one-at-a-time
  - If the threads share a lot of data and fit into cache, maybe SMT
    performance doesn't have as many issues.

/[CMP]: Chip Multiprocessor /[ARF]: Architectural Register File /[SMT]:
Simultaneous Multi-Threading /[SMP]: Symmetric Multiprocessor /[WS]:
Working Set /[D$]: Data Cache /[PIPT]: Physically Indexed Physically
Tagged /[TLB]: Translation Look-Aside Buffer /[VIPT]: Virtually Indexed
Physically Tagged /[VIVT]: Virtual Indexed Virtually Tagged /[ALU]:
Arithmetic Logic Unit /[IQ]: Instruction Queue /[LB]: Load Buffer /[LW]:
Load Word /[PC]: Program Counter /[RAT]: Register Allocation Table
(Register Alias Table) /[ROB]: ReOrder Buffer /[SB]: Store Buffer /[SW]:
Store Word /[UMA]: Uniform Memory Access *[NUMA]: Non-Uniform Memory
Access
